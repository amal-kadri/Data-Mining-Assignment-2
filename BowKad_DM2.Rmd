---
title: 'ECO 395 Homework 2: John Bowman, Amal Kadri'
author: "John Bowman, Amal Kadri"
date: "3/1/2022"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
library(tidyverse)
library(ggplot2)
library(rsample)
library(lubridate)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(readr)
library(data.table)
library(dplyr)
library(tibble)
library(zoo)
library(here)
library(mosaic)
library(gamlr)
library(class)


capmetro = read.csv(here("data", "capmetro_UT.csv"))
saratoga = (SaratogaHouses)
kredit = read.csv(here("data", "german_credit.csv"))
hotel_dev = read.csv(here("data", "hotels_dev.csv"))
hotel_val = read.csv(here("data", "hotels_val.csv"))
```
#1a
```{r setup, include=True}
knitr::opts_chunk$set(echo = FALSE)

capmetro = read.csv(here("data", "capmetro_UT.csv"))
capmetro = capmetro %>%
  mutate(date_time = ymd_hms(timestamp))

capmetro = capmetro %>% 
  mutate(Timestamp = timestamp)%>%
  separate(timestamp, c("Date","Time")  ,sep = " ")

capmetro$day_of_week = factor(capmetro$day_of_week, levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))

avg_board = capmetro %>% 
  group_by(hour_of_day, day_of_week, month) %>% 
  summarize(avg_pass = mean(boarding)) %>% 
  arrange(hour_of_day, day_of_week, month)
view(capmetro)

avg_board

ggplot(avg_board) +
  geom_line(aes(x = hour_of_day, y = avg_pass, col = month)) + 
  facet_wrap(~day_of_week) + ggtitle("Average Boardings") +
  xlab("Average Passengers") + ylab("Hour of Day")
  
```

#1b
```{r setup, include=True}

knitr::opts_chunk$set(echo = FALSE)

temp_board = capmetro %>%
  group_by(hour_of_day) %>%
  summarise(mean = mean(boarding))


ggplot(capmetro) +
  geom_point(aes(x=temperature, y = boarding, col = weekend), size = .05) +
  facet_wrap(~hour_of_day) + ggtitle("Average Boardings by Temperature") +
  xlab("Temperature") + ylab("Boardings")
```

#Number 2
```{r}
#Data Cleaning

saratoga = (SaratogaHouses)

saratoga$centralAir = ifelse(saratoga$centralAir == "Yes",1,0)
saratoga$centralAir = as.numeric(saratoga$centralAir)
saratoga = saratoga %>% 
  mutate(pubsewer = ifelse(sewer == "public/commercial", 1,0))
saratoga = saratoga %>% 
  mutate(septic = ifelse(sewer == "septic", 1,0))
saratoga = saratoga %>% 
  mutate(nosewer = ifelse(sewer == "none", 1,0))
saratoga = subset(saratoga, select = -c(sewer))
saratoga
#Lasso Regression

houseX = model.matrix(price~(.- pctCollege - pubsewer - septic - nosewer - waterfront - landValue - newConstruction -heating - fuel)^2, data = saratoga)[,-1] 

houseY = saratoga$price

houseLasso = gamlr(houseX, houseY, alpha = 1)
plot(houseLasso)
lambdamin = which.min(AICc(houseLasso))
houseCoef = coef(houseLasso)

#Soft Code Betas

coefData = houseCoef %>% 
  as.matrix() %>%
  as.data.frame()
coefData$magnitude = round(abs(coefData$seg100),2)
coefData
coefData = coefData %>%
  filter(magnitude > 0)
coefData = cbind(newColName = rownames(coefData), coefData)
coef_vector = as.vector(coefData[2:nrow(coefData),1])
f <- as.formula(
  paste("price", 
        paste(coef_vector, collapse = " + "), 
        sep = " ~ "))

#Optimal Lasso

lmLasso = eval(bquote(lm(.(f), data = saratoga)))

#Medium Linear Model

lm2 = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)

#KNN Split

saratoga_split = initial_split(saratoga, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

Xtrain = model.matrix(price ~ lotSize + livingArea  + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfront + newConstruction - 1, data=saratoga_train)

Xtest = model.matrix(price ~ lotSize + livingArea  + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfront + newConstruction - 1, data=saratoga_test)

ytrain = saratoga_train$price
ytest = saratoga_test$price

scale_train = apply(Xtrain, 2, sd)

folds_train_scale <- Xtrain %>% 
  scale(scale = scale_train) %>% 
  bind_cols(price = ytrain) %>% 
  as.data.frame(select(price, everything()))

folds_test_scale <- Xtest %>% 
  scale(scale = scale_train) %>% 
  bind_cols(price = ytest) %>% 
  as.data.frame(select(price, everything()))

k_grid = c(2:100)
K_folds = 5
houseKNN = crossv_kfold(folds_train_scale, k = K_folds)
houseKModel = map(houseKNN$train, ~ knnreg(price ~ lotSize + livingArea 
                                           + bedrooms + bathrooms + age, k= 20,
                                           data = .))
#models = map(houseKNN$train, ~ knnreg(price ~ lotSize, k=100, data = ., use.all=FALSE))

cv_House = foreach(k = k_grid, .combine = 'rbind')%do%{
  houseKModel = map(houseKNN$train, ~ knnreg(price ~ lotSize + livingArea 
                                           + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfrontYes + waterfrontNo + newConstructionNo, k=k,
                                           data = ., use.all = FALSE))
  houseErr = map2_dbl(houseKModel, houseKNN$test, modelr::rmse)
  c(k=k, err = mean(houseErr), std_err = sd(houseErr)/sqrt(K_folds))
  }

klist <- foreach(K = k_grid, .combine = 'c') %do%{
  knnreg(price ~ lotSize + age + landValue + livingArea + bedrooms + newConstructionNo, data=folds_train_scale, k=K) %>% 
    modelr::rmse(data = folds_test_scale)
  }
}



#houseMin = round(min(cv_House$err))
houseMinK = as.data.frame(cv_House) %>% arrange(err) %>% select(k) %>% head(1) %>% as.numeric()

minKModel = map(houseKNN$train, ~ knnreg(price ~ lotSize + livingArea 
      + bedrooms + bathrooms + age+ sewer
      + waterfront +newConstruction,k=houseMinK, data = .))

rmse(lm2, saratoga_test)
rmse(lmLasso, saratoga_test)
map2_dbl(minKModel, houseKNN$test, modelr::rmse)

```

```{r}

# KNN MODELS
nfold = 5

saratoga_folds <- SaratogaHouses %>%
  mutate(fold_id = rep(1:nfold, length=nrow(SaratogaHouses)) %>% sample)

rmse_cv <- foreach(fold = 1:nfold, .combine='c') %do% {
  # looping over fold_id
  folds_train <- saratoga_folds %>% 
    filter(fold_id != fold)
  folds_test <- saratoga_folds %>% 
    filter(fold_id == fold)
  
  # separate out response
  ytrain = folds_train$price
  ytest = folds_test$price
  
  # separate out features for scaling by sd of TRAIN set
  xtrain = model.matrix(~ . - 1 - price, data=folds_train)
  xtest = model.matrix(~ . - 1 - price, data=folds_test)
  
  scale_train = apply(xtrain, 2, sd)
  
  # TRAIN
  # a pipe that:
  #   scales TRAINING features by TRAINING sds
  #   recombines scaled features with un-scaled TRAIN y response 
  folds_train_scale <- xtrain %>% 
    scale(scale = scale_train) %>% 
    bind_cols(price = ytrain) %>% 
    as.data.frame(select(price, everything()))
  
  # TEST
  # a pipe that:
  #   scales TESTING features by TRAINING sds
  #   recombines scaled features with un-scaled TEST y response
  folds_test_scale <- xtest %>% 
    scale(scale = scale_train) %>%
    bind_cols(price = ytest) %>% 
    as.data.frame(select(price, everything()))
  
  klist <- foreach(K = 2:100, .combine = 'c') %do%{
  knnreg(price ~ lotSize + age + landValue + livingArea + bedrooms + newConstructionNo, data=folds_train_scale, k=K) %>% 
    modelr::rmse(data = folds_test_scale)
  }
}

rmse_matrix <- as.data.frame(rmse_cv) %>% 
  mutate(fold = case_when(row_number() < 100 ~ 1, 
                          row_number() >= 100 & row_number() < 200 ~ 2, 
                          row_number() >= 200 & row_number() < 300 ~ 3, 
                          row_number() >= 300 & row_number() < 400 ~ 4, 
                          row_number() >= 400 & row_number() < 500 ~ 5), 
         k = case_when(row_number() < 100 ~ row_number() + 1, 
                      row_number() >= 100 & row_number() < 200 ~ row_number() - 99, 
                      row_number() >= 200 & row_number() < 300 ~ row_number() - 199, 
                      row_number() >= 300 & row_number() < 400 ~ row_number() - 299, 
                      row_number() >= 400 & row_number() < 500 ~ row_number() - 399)) %>% 
  group_by(k) %>% 
  summarise(mean_rmse = mean(rmse_cv)) %>% 
  arrange(mean_rmse)

k_optimal <- rmse_matrix$k[1]
```



#Number 3
```{r}
kredit = read.csv(here("data", "german_credit.csv"))
kredit$Good = ifelse(kredit$history == "good",1,0)
kredit$Poor = ifelse(kredit$history == "poor",1,0)
kredit$Terrible = ifelse(kredit$history == "terrible",1,0)
#view(kredit)
credHistory = kredit%>%
  group_by(history)%>%
  summarise(numDefault = sum(Default), count = n()) %>%
  mutate(default_liklihood = numDefault/count)


ggplot(credHistory)+
  geom_col(aes(x = history, y = default_liklihood))

#glm
kredit_split = initial_split(kredit, prop = 0.8)
kredit_train = training(kredit_split)
kredit_test = testing(kredit_split)
credit_model = glm(Default ~ duration + amount + installment + age + history 
                   + purpose + foreign, family = "binomial", kredit_train)

kredit_test = mutate(.data = kredit_test, yhat = predict(credit_model, kredit_test, type='response'))
kredit_test
# ggplot(kred_test) + 
#   geom_jitter(aes(x=Default, y=yhat), width=0.1, alpha=0.2) + 
#   labs(title="Test-set predicted probabilities", y = "P(spam | x)", x="Spam?") + 
#   stat_summary(aes(x=Default, y=yhat), fun='mean', col='red', size=1)

credPredict = predict(credit_model, data=kredit_test, type="response")
yhat_test = ifelse(credPredict > 0.5, 1, 0)
kredit_test =  mutate(.data = kredit_test, yhat_t = ifelse(yhat > 0.5, 1, 0))
confusion_out_logit = table(y = kredit_test$Default,
                            yhat = kredit_test$yhat_t)

confusion_out_logit

#make ROC curve

```

```{r}
hotel_dev = read.csv(here("data", "hotels_dev.csv"))
hotel_val = read.csv(here("data", "hotels_val.csv"))

hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)

#Basic Model
model1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, family = "binomial", hotel_dev_train)
hotel_dev_test = mutate(.data = hotel_dev_test, yhat = predict(model1, hotel_dev_test, type='response')) %>% 
  mutate(.data = hotel_dev_test, childPred1 =  ifelse(yhat1 > 0.2, 1, 0))

confusedBasicChild = table(y = hotel_dev_test$children , yhat = hotel_dev_test$childPred)
# kredit_test =  mutate(.data = kredit_test, yhat_t = ifelse(yhat > 0.5, 1, 0))
# confusion_out_logit = table(y = kredit_test$Default,
#                             yhat = kredit_test$yhat_t)
confusedBasicChild

#Big Model
model2 = glm(children ~. - arrival_date, family = "binomial", hotel_dev_train)
hotel_dev_test = mutate(.data = hotel_dev_test, yhat2 = predict(model2, hotel_dev_test, type='response')) %>% 
  mutate(.data = hotel_dev_test, childPred2 =  ifelse(yhat2 > 0.5, 1, 0))
confusedBigChild = table(y = hotel_dev_test$children, yhat = hotel_dev_test$childPred2)
confusedBigChild

#Best Model (Lasso)
# scx = model.matrix(FAIL ~ .-1, data=semiconductor) # do -1 to drop intercept!
# scy = semiconductor$FAIL
# 
# # Note: there's also a "sparse.model.matrix"
# # here our matrix isn't sparse.
# # but sparse.model.matrix is a good way of doing things if you have factors.
# 
# # fit a single lasso
# sclasso = gamlr(scx, scy, family="binomial")


childX = model.matrix(children ~ .-arrival_date -1, data = hotel_dev_train)
childY = hotel_dev_train$children
childMatrix= t(model.matrix(children ~ .-arrival_date -1, data = hotel_dev_test))
childLasso = gamlr(childX,childY, family = "binomial")
childPred3 = predict(childLasso, childMatrix, select="min")
childCoef = coef(childLasso)
plot(childLasso)
childCV = cv.gamlr(childX,childY, nfold = 10, standardize = FALSE,
                   verb = TRUE,  family = "binomial")


#plot(childCV)
#childCoef

```
#lasso testing
```{r}
library(glmnet)
hotel_dev = read.csv(here("data", "hotels_dev.csv"))
hotel_val = read.csv(here("data", "hotels_val.csv"))
hotel_dev$meal =  as.factor(hotel_dev$meal)
hotel_dev$market_segment =  as.factor(hotel_dev$market_segment)
hotel_dev$distribution_channel =  as.factor(hotel_dev$distribution_channel)
hotel_dev$reserved_room_type =  as.factor(hotel_dev$reserved_room_type)
hotel_dev$assigned_room_type =  as.factor(hotel_dev$assigned_room_type)
hotel_dev$deposit_type =  as.factor(hotel_dev$deposit_type)
hotel_dev$customer_type =  as.factor(hotel_dev$customer_type)
hotel_dev$required_car_parking_spaces =  as.factor(hotel_dev$required_car_parking_spaces)

hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)
# #Define predictor and response variables
# y <- mtcars$hp
# x <- data.matrix(mtcars[, c('mpg', 'wt', 'drat', 'qsec')])
childY <- hotel_dev_train$children
childX <- model.matrix(children ~ .-arrival_date -1, data = hotel_dev_train)
childMatrix= t(model.matrix(children ~ .-arrival_date-1, data = hotel_dev_test))
randomcrap = model_matrix(children ~ reserved_room_type, data = hotel_dev_test)
# #fit lasso regression model using k-fold cross-validation
# cv_model <- cv.glmnet(x, y, alpha = 1)
# best_lambda <- cv_model$lambda.min
childLasso <- cv.glmnet(x = childX,y = childY ,alpha = 1, family = "binomial", numfold = 5, trace.it = 1, standardize = FALSE)
best_lambda <- childLasso$lambda.min

# #display optimal lambda value
# best_lambda

# #view plot of test MSE's vs. lambda values
# plot(cv_model)
plot(childLasso)
# #view coefficients of best model
# best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
# coef(best_model)
################################

childBest <- glmnet(childX,childY,alpha = 1, lambda = best_lambda, family = "binomial", numfold = 5, trace.it = 1, standardize = FALSE)
BetaKids = as.matrix(childBest[["beta"]])
testMatrix = model.matrix(children ~ .-arrival_date -1, data = hotel_dev_test)
predictions = predict(childBest, newx = testMatrix)


################################
# traintest=rbind(training,testing)
# X = sparse.model.matrix(as.formula(paste("y ~", paste(colnames(training[,-1]), sep = "", collapse=" +"))), data = traintest)
# model = cv.glmnet(X[1:nrow(training),], training[,1], family = "binomial",type.measure = "auc",nfolds = 10)
# plot(model)
# model$lambda.min
# #predict on test set
# pred = predict(model, s='lambda.min', newx=X[-(1:nrow(training)),], type="response")

# #make a prediction for the response value of a new observation
# new = matrix(c(24, 2.5, 3.5, 18.5), nrow=1, ncol=4) 
# predict(best_model, s = best_lambda, newx = new)
# 
# #find R-squared of model on training data
# y_predicted <- predict(best_model, s = best_lambda, newx = x)
# 
# sst <- sum((y - mean(y))^2)
# sse <- sum((y_predicted - y)^2)
# 
# rsq <- 1 - sse/sst
# rsq
```
#saved code
```{r}

traintest = rbind(hotel_dev_train, hotel_dev_test)
X = sparse.model.matrix(as.formula(paste(children ~ paste(colnames(hotel_dev_train[,-1]), sep = "", collapse=" +"))), data = traintest)
modelChild = cv.glmnet(X[1:nrow(hotel_dev_train),], hotel_dev_train[,1], family = "binomial", type.measure = "auc", nfolds = 10)
modelChild$lambda.min
plot(modelChild)
predChild = predict(modelChild, s = 'lambda.min', newx = X[-(1:nrow(hotel_dev_train)),], type = "response")
rmse(predChild)
pred = predict.(model, s='lambda.min', newx=X[-(1:nrow(training)),], type="response")
```



