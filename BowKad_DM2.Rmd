---
title: 'ECO 395 Homework 2: John Bowman, Jacob Bulzak, Amal Kadri'
author: "John Bowman, Jacob Bulzak, Amal Kadri"
date: "3/1/2022"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r lib, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(ggplot2)
library(rsample)
library(lubridate)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(readr)
library(data.table)
library(dplyr)
library(tibble)
library(zoo)
library(here)
library(mosaic)
library(gamlr)
library(class)
library(glmnet)
library(fastDummies)
library(ROCR)


capmetro = read.csv(here("data", "capmetro_UT.csv"))
saratoga = (SaratogaHouses)
kredit = read.csv(here("data", "german_credit.csv"))
hotel_dev = read.csv(here("data", "hotels_dev.csv"))
hotel_val = read.csv(here("data", "hotels_val.csv"))
```
#1-a
```{r #1-a, include=TRUE}
knitr::opts_chunk$set(echo = F)

#clean data

capmetro = read.csv(here("data", "capmetro_UT.csv"))
capmetro = capmetro %>%
  mutate(date_time = ymd_hms(timestamp))

capmetro = capmetro %>% 
  mutate(Timestamp = timestamp)%>%
  separate(timestamp, c("Date","Time")  ,sep = " ")

capmetro$day_of_week = factor(capmetro$day_of_week, levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))

#group data

avg_board = capmetro %>% 
  group_by(hour_of_day, day_of_week, month) %>% 
  summarize(avg_pass = mean(boarding), .groups = 'drop') %>% 
  arrange(hour_of_day, day_of_week, month)

#plot data

ggplot(avg_board) +
  geom_line(aes(x = hour_of_day, y = avg_pass, col = month)) + 
  facet_wrap(~day_of_week) + ggtitle("Average Boardings") +
  xlab("Average Passengers") + ylab("Hour of Day")
  
```
Caption: The plot above displays average boardings of Capital Metro buses on the UT campus, averaged by hour of day, broken down by month (September, October, November) and finally faceted by day of the week, and finally by month (September, October, November). In general, the hour of peak boardings fluctuates between 3PM and 5:30PM across weekdays.

This pattern is most likely the result of the majority of UT classes ending in the afternoon between these hours. We note that on weekends, there is no readily discernible peak boarding hour, since no classes are held on weekends and there will be much fewer students on campus. Average boardings on Mondays in September appear to be lower than in other days and months. A possible reason for this is twofold. First, given that the semester starts in late August and begins with syllabus week, many students may choose to not go to campus an attend classes as academic pressure is still low at this stage of the term. Second, having just arrived at school, many students may be excited to see their peers and spend their weekends partying (intensely) which may lower enthusiasm for Monday classes.

Similarly, we observe that average boardings on Wednesday, Thursday, and Friday appear lower in November. This is potentially caused by the fact that in November most courses hold their midterms. Furthermore, the majority of class assignments are likely due towards the end of the week. This greater workload may compel some students to stay on campus and study late into the night, causing them to miss the last bus and return home using other means.
#1-b
```{r #1-b, include=TRUE}
knitr::opts_chunk$set(echo = F)

#plot data

ggplot(capmetro) +
  geom_point(aes(x=temperature, y = boarding, col = weekend), size = .05) +
  facet_wrap(~hour_of_day) + ggtitle("Average Boardings by Temperature") +
  xlab("Temperature") + ylab("Boardings")
```
Caption: The plot above shows boardings of Capital Metro buses in each 15-mimnute window, by temperature, faceted by hour of day. Red and cyan points correspond to data from weekdays and weekends, respectively.

Suppose we hold weekend status and the hour of the day constant and examine all the data points for any one hour of the day. In this case, temperature does not seem to have much of an effect on the number of students riding the bus. While the number of boardings changes considerably during the day, in any one-hour block, the cluster of data points does not seem to exhibit any apparent trend with respect to temperature.

#2
```{r #2, include=FALSE}
knitr::opts_chunk$set(echo = F)

#data cleaning

saratoga = (SaratogaHouses)

saratoga$centralAir = ifelse(saratoga$centralAir == "Yes",1,0)
saratoga$centralAir = as.numeric(saratoga$centralAir)
saratoga = saratoga %>% 
  mutate(pubsewer = ifelse(sewer == "public/commercial", 1,0))
saratoga = saratoga %>% 
  mutate(septic = ifelse(sewer == "septic", 1,0))
saratoga = saratoga %>% 
  mutate(nosewer = ifelse(sewer == "none", 1,0))
saratoga = subset(saratoga, select = -c(sewer))

#KNN split

saratoga_split = initial_split(saratoga, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

# stepwise selection to pick the best slate of tripple interactions

lmbase = lm(price ~ ., data = saratoga_test)
lmstep = step(lmbase, scope=~(.)^3)

#medium linear model

lm2 = lm(price ~ . - pctCollege - pubsewer - septic - nosewer - waterfront - landValue - newConstruction, data=saratoga_train)

#KNN scale

Xtrain = model.matrix(price ~ lotSize + livingArea  + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfront + newConstruction - 1, data=saratoga_train)

Xtest = model.matrix(price ~ lotSize + livingArea  + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfront + newConstruction - 1, data=saratoga_test)

ytrain = saratoga_train$price
ytest = saratoga_test$price

scale_train = apply(Xtrain, 2, sd)

folds_train_scale <- Xtrain %>% 
  scale(scale = scale_train) %>% 
  bind_cols(price = ytrain) %>% 
  as.data.frame(select(price, everything()))

folds_test_scale <- Xtest %>% 
  scale(scale = scale_train) %>% 
  bind_cols(price = ytest) %>% 
  as.data.frame(select(price, everything()))

#k fold cross validation

k_grid = c(2:100)
K_folds = 5
houseKNN = crossv_kfold(folds_train_scale, k = K_folds)
houseKModel = map(houseKNN$train, ~ knnreg(price ~ lotSize + livingArea
                                           + bedrooms + bathrooms + age, k= 20,
                                           data = .))

cv_House = foreach(k = k_grid, .combine = 'rbind')%do%{
  houseKModel = map(houseKNN$train, ~ knnreg(price ~ (lotSize + livingArea 
                                           + bedrooms + bathrooms + age + pubsewer + septic + nosewer + waterfrontYes
                                           + waterfrontNo + newConstructionNo)^2, k=k,
                                           data = ., use.all = FALSE))
  houseErr = map2_dbl(houseKModel, houseKNN$test, modelr::rmse)
  c(k=k, err = mean(houseErr), std_err = sd(houseErr)/sqrt(K_folds))
  }

houseMinK = as.data.frame(cv_House) %>% arrange(err) %>% select(k) %>% head(1) %>% as.numeric()

minKModel = map(houseKNN$train, ~ knnreg(price ~ (lotSize + livingArea 
      + bedrooms + bathrooms + age+ pubsewer + septic + nosewer
      + waterfrontNo + waterfrontYes +newConstructionNo)^2,k=houseMinK, data = .))

# RMSE
#CV Medium
nFolds = 5
rmseLm2 = foreach(1:nFolds, .combine = 'rbind')%do%{
  saratoga_split = initial_split(saratoga, prop = 0.8)
  saratoga_train =  training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  lm2 = lm(price ~ . - pctCollege - pubsewer - septic - nosewer - waterfront - landValue - newConstruction, data=saratoga_train)
  lmErr = rmse(lm2, saratoga_test)
}
#CV Step
LmStep = lm(price ~ age + landValue + livingArea + bedrooms + 
    fireplaces + bathrooms + rooms + heating + waterfront + newConstruction + 
    centralAir + pubsewer + septic + landValue:centralAir + landValue:fireplaces + 
    bathrooms:heating + rooms:pubsewer + rooms:septic + fireplaces:bathrooms + 
    bedrooms:pubsewer + fireplaces:rooms, data = saratoga_test)

rmseLmStep = foreach(1:nFolds, .combine = 'rbind')%do%{
  saratoga_split = initial_split(saratoga, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  LmStep = lm(price ~ age + landValue + livingArea + bedrooms + 
    fireplaces + bathrooms + rooms + heating + waterfront + newConstruction + 
    centralAir + pubsewer + septic + landValue:centralAir + landValue:fireplaces + 
    bathrooms:heating + rooms:pubsewer + rooms:septic + fireplaces:bathrooms + 
    bedrooms:pubsewer + fireplaces:rooms, data = saratoga_train)
  lmErr = rmse(LmStep, saratoga_test)
}
```
```{r #Number 2 Result, include=TRUE}
knitr::opts_chunk$set(echo = F)
#Cross-validated lm2 RMSE
mean(rmseLm2)

#Cross-validated lmstep RMSE
mean(rmseLmStep)

#Cross-validated KNN RMSE
#map2_dbl(minKModel, houseKNN$test, modelr::rmse)
mean(map2_dbl(minKModel, houseKNN$test, modelr::rmse))

```
The highest performing model is the step-wise selected linear model. This is likely due to KNNs inability to handle many independent variables, where the step wise function is able to systematically remove low performing variables and keep high performing variables. Some interactions might be adding variance without better explaining the data, while others increase the models explanatory power. The KNN model can't get rid of the noise created by all the interactions while the step-wise gradually eliminates variables till the model yielding the lowest RMSE is produced. Having multiple favorable features in a property like waterfront access and proximity to schools can increase the value of a property exponentially, but the presence of some pairs of features are much more significant than other pairs, so choosing a method of modeling that can remove these non significant pairs (or interactions) is crucial.

#3
```{r #3, include=TRUE}
knitr::opts_chunk$set(echo = F)

#clean data

kredit = read.csv(here("data", "german_credit.csv"))
kredit$Good = ifelse(kredit$history == "good",1,0)
kredit$Poor = ifelse(kredit$history == "poor",1,0)
kredit$Terrible = ifelse(kredit$history == "terrible",1,0)

#group data

credHistory = kredit%>%
  group_by(history)%>%
  summarise(numDefault = sum(Default), count = n()) %>%
  mutate(default_liklihood = numDefault/count)

ggplot(credHistory)+
  geom_col(aes(x = history, y = default_liklihood)) + ggtitle("Default Likelihood by Credit History") +
  xlab("Credit History") + ylab("Default Likelihood")
```
The bar plot above displays the proportion of total of defaults for three credit history classes, “good”, “poor”, and “terrible”. Intuitively, we would expect that a worse credit history corresponds to a higher default rate. Surprisingly, we observe the exact opposite of this relationship. Borrowers with good credit history exhibit the highest default rate, whereas those with terrible credit history exhibit the lowest default rate of the three categories. This counterintuitive result likely stems from how to dataset was constructed. As defaults are overall rather rare, the German bank whose data we analyzed decided to sampled a set of loans that had defaulted, and attempted to match this set with similar loans that had not defaulted.
```{r #3-a, include=TRUE}
#train test split

kredit_split = initial_split(kredit, prop = 0.8)
kredit_train = training(kredit_split)
kredit_test = testing(kredit_split)

#glm

credit_model = glm(Default ~ duration + amount + installment + age + history 
                   + purpose + foreign, family = "binomial", kredit_train)

kredit_test = mutate(.data = kredit_test, yhat = predict(credit_model, kredit_test, type='response'))

#model predictions

credPredict = predict(credit_model, data=kredit_test, type="response")
yhat_test = ifelse(credPredict > 0.5, 1, 0)
kredit_test =  mutate(.data = kredit_test, yhat_t = ifelse(yhat > 0.5, 1, 0))
confusion_out_logit = table(y = kredit_test$Default,
                            yhat = kredit_test$yhat_t)

confusion_rate = (confusion_out_logit/sum(confusion_out_logit))

confusiontable <- as.data.frame(confusion_rate)

newcol <- c('True Negative', 'False Negative', 'False Positive', 'True Positive')

nct <- cbind(confusiontable, newcol)
nct %>%
  rename(Frequency = Freq, Result = newcol)

```
The set of loans that defaulted probably had a higher-than-average proportion of loans with terrible credit history. Because this sample had many more loans, it likely also had a broader set of characteristics allowing matching with sample loans that did not default. The list of loans with good credit history that defaulted, was much smaller, (as individuals with good credit history do not often default on their loans) and thus did was less likely to match with loans that did not default. Hence by matching loans in such a way, the bank unintentionally generated an inverse relationship between credit history and default likelihood. The bank should thus change its sampling scheme. For example, random sampling of loans would provide a better set of data, as it would be a more accurate representation of the overall portfolio.

#4-a
```{r #4-a, include=TRUE}
knitr::opts_chunk$set(echo = F)

#clean data

library(fastDummies)
hotel_dev = read.csv(here("data", "hotels_dev.csv"))

hotel_dev$arrival_date =  ymd(hotel_dev$arrival_date)
storeDev = hotel_dev
DumbHotels = dummy_cols(hotel_dev)
DumbDev = DumbHotels%>%
  mutate(reserved_room_type_I = 0) %>%
  mutate(reserved_room_type_J = 0) %>%
  mutate(reserved_room_type_K = 0) %>%
  mutate(reserved_room_type_L = 0) %>%
  mutate(assigned_room_type_J = 0) %>%
  mutate(assigned_room_type_L = 0)

DumbDev = DumbDev %>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment,
            distribution_channel, deposit_type, customer_type, required_car_parking_spaces))

hotel_dev = DumbDev 

mainVariables = colnames(model_matrix(children ~ (.-arrival_date)^2 -1, data = hotel_dev))
 

#train-test splits
hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)

#Matrices
childY <- hotel_dev_train$children
childX <- model.matrix(children ~ (.-arrival_date)^2 -1, data = hotel_dev_train)

childLasso <- cv.glmnet(x = childX,y = childY ,alpha = 1, family = "binomial", nfold = 5, trace.it = 1, standardize = FALSE)

testMatrix = model.matrix(children ~ (.-arrival_date)^2 -1, data = hotel_dev_test)

hotel_dev_test = mutate(.data = hotel_dev_test, childPred3 = predict(childLasso, newx = testMatrix, type = "class", s = 'lambda.min'))
confusedBestChild = table(y = hotel_dev_test$children, yhat_Lasso = hotel_dev_test$childPred3)

#Basic Model

hotel_dev = storeDev

hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)

model1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, family = "binomial", hotel_dev_train)

hotel_dev_test = mutate(.data = hotel_dev_test, yhat1 = predict(model1, hotel_dev_test, type='response')) %>% 
  mutate(.data = hotel_dev_test, childPred1 =  ifelse(yhat1 > 0.2, 1, 0))

confusedBasicChild = table(y = hotel_dev_test$children , yhat = hotel_dev_test$childPred1)


#Big Model

model2 = glm(children ~. - arrival_date, family = "binomial", hotel_dev_train)

hotel_dev_test = mutate(.data = hotel_dev_test, yhat2 = predict(model2, hotel_dev_test, type='response')) %>% 
  mutate(.data = hotel_dev_test, childPred2 =  ifelse(yhat2 > 0.5, 1, 0))

confusedBigChild = table(y = hotel_dev_test$children, yhat_Logit = hotel_dev_test$childPred2)

confusedBasicChild

confusedBigChild

confusedBestChild

```


#4-b
```{r #4-b, include=FALSE}
knitr::opts_chunk$set(echo = F)

#clean data

hotel_val = read.csv(here("data", "hotels_val.csv"))
hotel_val$arrival_date = ymd(hotel_val$arrival_date)

#otherVariables = colnames(model_matrix(children ~ (.-arrival_date)^2 -1, data = hotel_val))

DumbVal = dummy_cols(hotel_val)
DumbVal = DumbVal%>%
  mutate(reserved_room_type_I = 0) %>%
  mutate(reserved_room_type_J = 0) %>%
  mutate(reserved_room_type_K = 0) %>%
  mutate(reserved_room_type_L = 0) %>%
  mutate(assigned_room_type_J = 0) %>%
  mutate(assigned_room_type_L = 0)

DumbVal = DumbVal %>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment,
            distribution_channel, deposit_type, customer_type, required_car_parking_spaces))
hotel_val = DumbVal

#train-test splits

hotel_val_split = initial_split(hotel_val, prop = 0.8)
hotel_val_train = training(hotel_val_split)
hotel_val_test = testing(hotel_val_split)


#test matrix

valTestMatrix = model.matrix(children ~ (.-arrival_date)^2 - 1, data = hotel_val_test)

hotel_val_test = mutate(.data = hotel_val_test,
                        childProb3 = predict(childLasso, newx=valTestMatrix,
                                             type = "response", s = 'lambda.min'))

hotel_val_test = hotel_val_test %>% mutate(.data = hotel_val_test, childPred3 =  ifelse(childProb3 > 0.5, 1, 0))

confusedBestChild = table(y = hotel_val_test$children, yhat_Lasso = hotel_val_test$childPred3)

confusedBestChild

hotel_val_test
```


```{r #4-b-output, include=TRUE}
knitr::opts_chunk$set(echo = F)
# roc curve

thresh_grid = seq(0.90, 0.05, by=-0.005)

roc_curve_child = foreach(thresh = thresh_grid, .combine='rbind') %do% {
  child_test = ifelse(hotel_val_test$childProb3 >= thresh, 1, 0)
  confusion_out_logit = table(y = hotel_val_test$children, yhat = child_test)
  out_logit = data.frame(model = "logit",
                         TPR = confusion_out_logit[2,2]/sum(hotel_val_test$children==1), 
                         FPR = confusion_out_logit[1,2]/sum(hotel_val_test$children==0))
  rbind(out_logit)
} %>% as.data.frame()

ggplot(roc_curve_child) + 
  geom_line(aes(x=FPR, y=TPR)) + 
  labs(title="ROC Legit Lasso") +
  theme_bw(base_size = 10)


```

#4-c
```{r #4-c, include=TRUE}
knitr::opts_chunk$set(echo = F)

#clean data

hotel_val = read.csv(here("data", "hotels_dev.csv"))

hotel_val$arrival_date =  ymd(hotel_val$arrival_date)
storeVal = hotel_val
DumbHotels = dummy_cols(hotel_val)
DumbVal = DumbHotels%>%
  mutate(reserved_room_type_I = 0) %>%
  mutate(reserved_room_type_J = 0) %>%
  mutate(reserved_room_type_K = 0) %>%
  mutate(reserved_room_type_L = 0) %>%
  mutate(assigned_room_type_J = 0) %>%
  mutate(assigned_room_type_L = 0)

DumbVal = DumbVal %>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment,
            distribution_channel, deposit_type, customer_type, required_car_parking_spaces))

hotel_val = DumbVal

#train-test splits

hotel_val_split = initial_split(hotel_val, prop = 0.8)
hotel_val_train = training(hotel_val_split)
hotel_val_test = testing(hotel_val_split)

#matrices

childY <- hotel_val_train$children
childX <- model.matrix(children ~ (.-arrival_date)^2 -1, data = hotel_val_train)

childLasso <- cv.glmnet(x = childX,y = childY ,alpha = 1, family = "binomial", nfold = 20, trace.it = 1, standardize = FALSE)

child_coef = coef(childLasso) %>% 
  as.matrix() %>% 
  as.data.frame()

child_coef = child_coef %>% 
  mutate(mag = abs(s1)) %>% 
  filter(mag > 0)

child_coef <- tibble::rownames_to_column(child_coef, "VALUE")

child_coef = child_coef[2:nrow(child_coef),1]
child_coef

f <- as.formula(
  paste("children", 
        paste(child_coef, collapse = " + "), 
        sep = " ~ "))


testMatrix = model.matrix(children ~ (.-arrival_date)^2 -1, data = hotel_val_test)

hotel_val_test = mutate(.data = hotel_val_test, childPred3 = predict(childLasso, newx = testMatrix, type = "class", s = 'lambda.min'))

confusedBestChild = table(y = hotel_val_test$children, yhat_Lasso = hotel_val_test$childPred3)

confusedBestChild

```

```{r #4-c-1, include=TRUE}
knitr::opts_chunk$set(echo = F)
#clean data

hotel_val = read.csv(here("data", "hotels_dev.csv"))
#view(hotel_val)
hotel_val$arrival_date =  ymd(hotel_val$arrival_date)
storeVal = hotel_val
DumbHotels = dummy_cols(hotel_val)
DumbVal = DumbHotels%>%
  mutate(reserved_room_type_I = 0) %>%
  mutate(reserved_room_type_J = 0) %>%
  mutate(reserved_room_type_K = 0) %>%
  mutate(reserved_room_type_L = 0) %>%
  mutate(assigned_room_type_J = 0) %>%
  mutate(assigned_room_type_L = 0)

DumbVal = DumbVal %>%
  select(-c(assigned_room_type, reserved_room_type, hotel, meal, market_segment,
            distribution_channel, deposit_type, customer_type, required_car_parking_spaces))

hotel_val = DumbVal

#train-test splits

hotel_val_split = initial_split(hotel_val, prop = 0.8)
hotel_val_train = training(hotel_val_split)
hotel_val_test = testing(hotel_val_split)

K_folds = 20
hotel_val = hotel_val%>%
  mutate(fold_id = rep(1:K_folds, length=nrow(hotel_val)) %>% sample)

folds_lasso = foreach(fold = 1:K_folds, .combine='c') %do% {
  in_fold_data = filter(hotel_val, fold_id == fold)
  out_fold_data = filter(hotel_val, fold_id != fold)
  x=model.matrix(children ~ (. -arrival_date - 1 ), data=out_fold_data)
  y=out_fold_data$children
  lasso = cv.gamlr(x, y,nfold=5, family="binomial")
  xval=model.matrix(children ~ (. -arrival_date - 1), data=in_fold_data)
  pred = predict(lasso, xval, type= "response")
  yhat_val = ifelse(pred >= 0.5, 1, 0)
  table = table(y=in_fold_data$children, yhat=yhat_val)
  TPR = table[2,2] / (table[2,1] + table[2,2]) #TPR
}


folds_lasso = data.frame(folds_lasso)
colnames(folds_lasso)=c("TPR")
folds_lasso$folds = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
folds_lasso
meanTPR = mean(folds_lasso$TPR)
####################################################################
```


```{r #Number 4foldfinal, include=TRUE}
knitr::opts_chunk$set(echo = F)
meanTPR
TPR_fold_comparison = ggplot(folds_lasso)+
  geom_col(aes(x=folds, y=TPR))+labs(
    x="Fold",
    y="TPR",
    title = "TPR by Fold")
TPR_fold_comparison
```

```{r #Number defunct code, include=TRUE}
#defunct code ##########################
knitr::opts_chunk$set(echo = F)
# `%notin%` <- Negate(`%in%`)

# f_include = function(mainVariables, variable_list, varaible_matrix) 
#   for (i in mainVariables) {
#     if (mainVariables[i] %notin% variable_list) {
#       mutate(variable_matrix$mainVariables[i] <- 0)}
#     }

# child_coef = coef(childLasso) %>% 
#   as.matrix() %>% 
#   as.data.frame()
# 
# child_coef = child_coef %>% 
#   mutate(mag = abs(s1)) %>% 
#   filter(mag > 0)
# 
# child_coef <- tibble::rownames_to_column(child_coef, "VALUE")
# 
# child_coef = child_coef[2:nrow(child_coef),1]
# child_coef

# f <- as.formula(
#   paste("children", 
#         paste(child_coef, collapse = " + "), 
#         sep = " ~ "))
```

